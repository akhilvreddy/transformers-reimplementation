{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905a0af2",
   "metadata": {},
   "source": [
    "# Transformer from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b1eee1",
   "metadata": {},
   "source": [
    "I reimplemented *Attention is All You Need* and built up the encoder/decoder layers very modular. I initally wrote a test suite to make sure that gradients are flowing and shapes are matching and then implemented each block independently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c71cd9",
   "metadata": {},
   "source": [
    "My goal for here is to make my model say somewhat coherent text. I'm using the famous shakespeare dataset (competely raw, no text cleaning) and aiming to use a decoder only network to make our model say words (just like ChatGPT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb0f43",
   "metadata": {},
   "source": [
    "First, I need to grab the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05cb1562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: Failed to open the file data/input.txt: No such file or directory\n",
      "  0 1089k    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "curl: (56) Failure writing output to destination, passed 1369 returned 4294967295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='curl -o data/input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt', returncode=56)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run(\"curl -o data/input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0887ae94",
   "metadata": {},
   "source": [
    "I also need to fix the global path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5893ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # transformer/ is up one level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa927a5d",
   "metadata": {},
   "source": [
    "We can start by calling the main imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ca6815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer.transformer import Transformer\n",
    "from transformer.decoder import Decoder\n",
    "from transformer.utils import CharDataset, generate_subsequent_mask\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966d3abb",
   "metadata": {},
   "source": [
    "Before we dive into the code, I want to make sure gradients are flowing, forward / backward passes are working, and shapes are matching (all covered with the test suite I wrote)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c54dfbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running test_attention.py...\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.13.2, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/akhilvreddy/Documents/transformers-reimplementation\n",
      "configfile: pytest.ini\n",
      "collected 2 items\n",
      "\n",
      "test_attention.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.46s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Running test_decoder.py...\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.13.2, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/akhilvreddy/Documents/transformers-reimplementation\n",
      "configfile: pytest.ini\n",
      "collected 1 item\n",
      "\n",
      "test_decoder.py \u001b[32m.\u001b[0m\u001b[32m                                                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.48s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Running test_encoder.py...\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.13.2, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/akhilvreddy/Documents/transformers-reimplementation\n",
      "configfile: pytest.ini\n",
      "collected 1 item\n",
      "\n",
      "test_encoder.py \u001b[32m.\u001b[0m\u001b[32m                                                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.49s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Running test_transformer.py...\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.13.2, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/akhilvreddy/Documents/transformers-reimplementation\n",
      "configfile: pytest.ini\n",
      "collected 1 item\n",
      "\n",
      "test_transformer.py \u001b[32m.\u001b[0m\u001b[32m                                                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.51s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Running test_utils.py...\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.13.2, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/akhilvreddy/Documents/transformers-reimplementation\n",
      "configfile: pytest.ini\n",
      "collected 2 items\n",
      "\n",
      "test_utils.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.44s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "os.chdir(\"../tests\")\n",
    "\n",
    "test_files = [\n",
    "    \"test_attention.py\",\n",
    "    \"test_decoder.py\",\n",
    "    \"test_encoder.py\",\n",
    "    \"test_transformer.py\",\n",
    "    \"test_utils.py\"\n",
    "]\n",
    "\n",
    "for test_file in test_files:\n",
    "    print(f\"\\nRunning {test_file}...\")\n",
    "    result = subprocess.run([\"pytest\", test_file], capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72925c55",
   "metadata": {},
   "source": [
    "We now have our data in `data/input.txt`, our test suite is passing, and our model is setup with the params I set in `config.yaml`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bff3b7",
   "metadata": {},
   "source": [
    "Let's load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2c3e5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "text = open(\"../data/input.txt\").read()\n",
    "block_size = 64\n",
    "\n",
    "dataset = CharDataset(text, block_size)\n",
    "vocab_size = dataset.vocab_size\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b824fd3",
   "metadata": {},
   "source": [
    "Our vocab size of 65 means that we have 65 *independent* characters that are going to be fed into the model. You can think of the ASCII characters (a, b, A, B, \\n, ., ?) that are each going to be treated like a unique token with its own embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f48ca",
   "metadata": {},
   "source": [
    "So this means that our model has an embedding layer of shape `(65, d_model)` and output logits are shaped `(B, T, 65)` which is one score for each possible character at vevery time step, for each input in the batch.\n",
    "\n",
    "During sampling I'll argmax or sample (beam / top k) from that 65-sized vector. To start with I'll argmax just to see if the text is cohesive and later I'll switch to other techniques that give better meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c245a",
   "metadata": {},
   "source": [
    "We can go ahead and initialize our decoder-only autoregrssive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aed96e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = yaml.safe_load(open(\"../config.yaml\"))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Decoder(\n",
    "    num_layers=cfg[\"num_layers\"],\n",
    "    d_model=cfg[\"d_model\"],\n",
    "    num_heads=cfg[\"num_heads\"],\n",
    "    dim_ff=cfg[\"dim_ff\"],\n",
    "    vocab_size=dataset.vocab_size,  # overwrite config to match actual dataset\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "431f6473",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(\n",
    "    num_layers=cfg[\"num_layers\"],\n",
    "    d_model=cfg[\"d_model\"],\n",
    "    num_heads=cfg[\"num_heads\"],\n",
    "    dim_ff=cfg[\"dim_ff\"],\n",
    "    vocab_size=vocab_size,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29a3df",
   "metadata": {},
   "source": [
    "Before we start with a full blown training loop, I want to make sure a single training step would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e0f27da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.40109395980835\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "# grab one (x, y) pair and move to device\n",
    "x, y = dataset[0]\n",
    "x = x.unsqueeze(0).to(device) # shape: (1, T)\n",
    "y = y.unsqueeze(0).to(device) # shape: (1, T)\n",
    "\n",
    "# get causal mask\n",
    "tgt_mask = generate_subsequent_mask(x.size(1)).to(device)\n",
    "\n",
    "# forward pass\n",
    "logits = decoder(x, enc_out=None, src_mask=None, tgt_mask=tgt_mask)\n",
    "\n",
    "# loss\n",
    "loss_fn = CrossEntropyLoss()\n",
    "loss = loss_fn(logits.view(-1, vocab_size), y.view(-1))\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# backward pass\n",
    "decoder.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# quick optimizer step to verify gradient flow\n",
    "optimizer = Adam(decoder.parameters(), lr=float(cfg[\"learning_rate\"]))\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd0b2ae",
   "metadata": {},
   "source": [
    "After a lot of debugging and editing my transformer so that it supports a decoder-only model, it finally returned a loss! The scalar value of the loss was **4.401**, which is pretty awful. I'm confident to throw the model in a training loop now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69a980a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 5/100 [00:00<00:04, 19.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Loss: 2.8867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 14/100 [00:00<00:04, 20.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 | Loss: 0.9605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 25/100 [00:01<00:03, 19.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20 | Loss: 0.8871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 34/100 [00:01<00:03, 20.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30 | Loss: 0.7898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 43/100 [00:02<00:02, 20.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40 | Loss: 0.9317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 55/100 [00:02<00:02, 20.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50 | Loss: 0.9714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 64/100 [00:03<00:01, 20.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 60 | Loss: 0.7389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 73/100 [00:03<00:01, 20.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 70 | Loss: 0.8672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 85/100 [00:04<00:00, 20.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 80 | Loss: 1.2584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 94/100 [00:04<00:00, 20.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 90 | Loss: 1.0607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:04<00:00, 20.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_steps = 100\n",
    "\n",
    "for step in tqdm(range(num_steps), desc=\"Training\"):\n",
    "    x, y = dataset[step]\n",
    "    x = x.unsqueeze(0).to(device)\n",
    "    y = y.unsqueeze(0).to(device)\n",
    "    tgt_mask = generate_subsequent_mask(x.size(1)).to(device)\n",
    "\n",
    "    logits = decoder(x, enc_out=None, src_mask=None, tgt_mask=tgt_mask)\n",
    "    loss = loss_fn(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    decoder.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4134c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(model, dataset, prompt, max_new_tokens=100):\n",
    "    model.eval()\n",
    "    stoi, itos = dataset.stoi, dataset.itos\n",
    "\n",
    "    # encode the prompt\n",
    "    idx = torch.tensor([stoi[c] for c in prompt], dtype=torch.long)[None].to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        tgt_mask = generate_subsequent_mask(idx.size(1)).to(device)\n",
    "        logits = model(idx, enc_out=None, src_mask=None, tgt_mask=tgt_mask)\n",
    "\n",
    "        # take the last time step\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "        # append to sequence\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    return ''.join([itos[i.item()] for i in idx[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2149ef9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kingzen:\n",
      "You are are alvesolvesolvesolvesolvesolvesolvesolvesolvesolverarararararararatolverererererererererererererererererererererererererererererererererererererereramiesolvesolvesolveramiesolvesolvera\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(decoder, dataset, prompt=\"The king\", max_new_tokens=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b0953c",
   "metadata": {},
   "source": [
    "It's amazing to see that the model generated text! The text looks horrible but it's cool to see that our decoder only model learned *something*. It went on a huge tangent repeating \"thamiso\" over and over but one cool thing to is that it learned punctuation well. Towards the end of the first line you can see that we have a \"?\" and right after we get a capital \"A\". That put a smile on my face since I could see that there were some attention heads paying attention to that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006bc557",
   "metadata": {},
   "source": [
    "I'm going to incrementally keep scaling up training. To generate that text, it took me ~10 seconds of training. I want to scale up so that the loop takes at least 2 to 3 minutes so that I can get the loss below 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d299df9",
   "metadata": {},
   "source": [
    "I'm going to make a bunch of changes here for easy training on my laptop. \n",
    "\n",
    "First, I'm starting with reducing the input size to only 50000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82e51f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 59\n"
     ]
    }
   ],
   "source": [
    "text = open(\"../data/input.txt\").read()\n",
    "text = text[:50000]\n",
    "block_size = 128\n",
    "\n",
    "dataset = CharDataset(text, block_size)\n",
    "vocab_size = dataset.vocab_size\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c731702b",
   "metadata": {},
   "source": [
    "I'm going to set up a DataLoader so we can do a full on training loop with batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dec21c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=cfg[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77993b41",
   "metadata": {},
   "source": [
    "I'm also going to upgrade to mps now because this training loop is actually quite large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2699a6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3473efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoder.to(device)\n",
    "\n",
    "# MPS needs this to prevent the embedding \"Placeholder\" error (thank you Claude)\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randint(0, vocab_size, (1, block_size)).to(device)\n",
    "    mask = generate_subsequent_mask(block_size).to(device)\n",
    "    _ = decoder(dummy, enc_out=None, src_mask=None, tgt_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bde91b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 780/780 [03:04<00:00,  4.23it/s, loss=0.1381]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 1\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = Adam(decoder.parameters(), lr=float(cfg[\"learning_rate\"]))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    pbar = tqdm(loader, total=len(loader), desc=\"Training\")\n",
    "\n",
    "    for x, y in pbar:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        tgt_mask = generate_subsequent_mask(x.size(1)).to(device)\n",
    "\n",
    "        logits = decoder(x, enc_out=None, src_mask=None, tgt_mask=tgt_mask)\n",
    "        # loss = loss_fn(logits.view(-1, vocab_size), y.view(-1))\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        decoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a67872b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The king fury, or go condition.\n",
      "\n",
      "AUFIDIUS:\n",
      "Condition!\n",
      "I would I were a Roman; for I cannot,\n",
      "Being a Volsce, be that I am. Condition!\n",
      "Where hough addites and one this last?\n",
      "\n",
      "MENENIUS:\n",
      "The then shalle be the sh\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(decoder, dataset, prompt=\"The king\", max_new_tokens=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34757f",
   "metadata": {},
   "source": [
    "That's real words! And it definitley does look like it's following same structure as shakespeare. I'm super bullish on this method now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b75a682",
   "metadata": {},
   "source": [
    "I'm also super grateful for having mps - it definitley sped up training massively. \n",
    "\n",
    "Let's redo the model with 2 or 4 epochs and see how much better the text generation can look like (before we go into token sampling techniques)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64e689",
   "metadata": {},
   "source": [
    "(I froze the 2 cells above and re-ran from the beginning of this so that we can start the model from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15b9e2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 780/780 [03:05<00:00,  4.20it/s, loss=0.1175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 780/780 [03:10<00:00,  4.10it/s, loss=0.1012]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 2\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = Adam(decoder.parameters(), lr=float(cfg[\"learning_rate\"]))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    pbar = tqdm(loader, total=len(loader), desc=\"Training\")\n",
    "\n",
    "    for x, y in pbar:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        tgt_mask = generate_subsequent_mask(x.size(1)).to(device)\n",
    "\n",
    "        logits = decoder(x, enc_out=None, src_mask=None, tgt_mask=tgt_mask)\n",
    "        # loss = loss_fn(logits.view(-1, vocab_size), y.view(-1))\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        decoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb47405",
   "metadata": {},
   "source": [
    "Time to see if this helped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a213d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kingly-crowned head, the vigilant eye,\n",
      "The counsellor heart, the arm our soldier,\n",
      "Our steed the leg, the tongue our trumpeter.\n",
      "With other muniments and petty helpere.\n",
      "\n",
      "SICINIUS:\n",
      "He the to the revere yedse\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(decoder, dataset, prompt=\"The king\", max_new_tokens=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129136e",
   "metadata": {},
   "source": [
    "So just *2 epochs in*, this looks clearly like shakespearean english. I'm confident our transformer is doing a good job at learn and the auto-regressive property is working really well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34417d85",
   "metadata": {},
   "source": [
    "I can keep scaling up the training loop now, but we can clearly see how the core architecture is working. I'm really happy with how the model was able to generate cohesive text just from character-level tokens. We can only imagine how much better this would be if we had better pre-processing, bpe level tokenization, and an actual GPU to train heavily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a1aa0c",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "Before I finish, I wanted to recap what I did. \n",
    "\n",
    "1) **Test suite**\n",
    "\n",
    "I struggled with shapes and masking logic issues in a previous verion so I wanted to fix that here. I started by writing test cases for attention (self and multi-head) to be calcualted properly and also wanted the shapes to be correct. I also had a bunch of util functions (mainly for help with masking) that I needed to make sure if they were returning the right shapes at the end. After I had test cases for those parts, writing tests for encoder, decoder, and transformer were pretty straightforward.\n",
    "\n",
    "2) **Writing blocks from scratch**\n",
    "\n",
    "I wrote embeddings and attention first. Embeddings wasn't that bad because I used PyTorch's native method for token embeddigns and I was able to refer to a lot of code online for positional encoding (since those calcualtions never change). \n",
    "\n",
    "After I had those down, writing the encoder, decoder, and trasnformer were super easy - it was just stacking the blocks we had already in multiple different ways.\n",
    "\n",
    "3) **Writing and tweaking my config.yaml**\n",
    "\n",
    "This was my most favorite part - it's super simple and I like having the ability to change everything about my model just from that yaml file. I played around with a bunch of different batch sizes and architecture settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0563ee1f",
   "metadata": {},
   "source": [
    "Throughout this project, I really learned the power of repeated *Attention + FF + LayerNorm* blocks - it's quite literally the strongest architecture I've ever worked with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
